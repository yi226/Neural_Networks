# 程序说明——神经网络

###### 人工智能2001 易鹏飞 U202014948

## 一. 运行环境，使用第三方库

运行环境为：python3.9

使用第三方库：

- numpy 建立模型，进行计算
- scikit-learn 下载数据集
- matplotlib 绘图

## 二. 全连接层神经网络(Neural_network.py)

### 1. 题目

使用numpy实现神经网络全连接层，完成手写数字mnist数据集上多分类任务

### 2. 类，方法，函数说明

#### 整个代码包括6个类和1个函数：

|         类/函数         |                  主要作用                   |
| :---------------------: | :-----------------------------------------: |
|     Linear(objetct)     |                 全连接层类                  |
|      Relu(objetct)      |        Relu激活函数类,引入非线性因素        |
|     Sigmoid(object)     |      Sigmoid激活函数类,引入非线性因素       |
| SoftmaxWithLoss(object) |      附带交叉熵损失的SoftMax类(输出层)      |
|       MLP(object)       |              构建多层感知机类               |
|      MNIST(object)      |              获得手写数字集类               |
|         main()          | 调用神经网络模型完成 mnist 数据集上分类任务 |

#### Linear类

- ##### \__init__方法：

  | 输入参数 |   含义   |
  | :------: | :------: |
  |   n_in   | 输入维度 |
  |  n_out   | 输出维度 |

  作用：初始化参数

- ##### \__call__方法：

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：调用forward方法

- ##### init_param方法：

  | 输入参数 |     含义     |
  | :------: | :----------: |
  | std=0.01 | 分布的标准差 |

  作用：初始化系数weight,bias

  权重矩阵weight 为以0为均值，标准差为std，shape为(n_in,n_out)的二维数组

  偏置向量bias 为shape为(1,n_out)的参数全为0的向量

- ##### forward方法：

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：全连接层前向传播(进行预测)计算

  数学模型：$Y = XW + b$  其中$Y$是输出，$X$是输入，$W$是权重矩阵，$b$是偏置向量。

  模型实现：

  ```python
  output = np.matmul(input, self.params['weight']) + self.params['bias']
  ```

  输出：$Y$

- ##### backward方法：

  | 输入参数 |          含义          |
  | :------: | :--------------------: |
  | preGrad  | 反向传播前一层的偏导数 |

  作用：全连接层反向传播(更新参数)计算

  数学模型：对forward方法中数学模型求偏导(梯度)

  $$ \left\{ \begin{matrix} \nabla_WL = X^T(\nabla_YL) \\ \nabla_bL = (1)\nabla_YL \\ \nabla_XL = (\nabla_YL)W^T  \end{matrix} \right. $$

  preGrad即为$\nabla_YL$  其中第二个公式左乘$(1)$是为了让b的偏导行维度变为1

  模型实现：

  ```python
  self.d_params['weight'] = np.dot(self.input.T, preGrad)
  self.d_params['bias'] = np.sum(preGrad, axis=0)
  postGrad = np.dot(preGrad, self.params['weight'].T)
  ```

  输出：$\nabla_XL$

- ##### update_param方法：

  | 输入参数 |    含义    |
  | :------: | :--------: |
  |    lr    |   学习率   |
  | momentum | 动量超参数 |

  作用：对全连接层参数进行更新(使用SGD + momentum)

  数学模型：

  $$ \left\{ \begin{matrix} V \leftarrow \alpha V-\eta(\nabla_WL) \\ W \leftarrow W + V  \end{matrix} \right. $$

  其中momentum为$\alpha$       lr为$\eta$

  通过动量的添加，使参数沿梯度方向加速下降，找到使损失函数的值尽可能小的参数

  模型实现：

  ```python
  self.pre[key] = momentum * self.pre[key] - lr * self.d_params[key]
  self.params[key] += self.pre[key]
  ```

- ##### load_param方法：

  | 输入参数 |   含义   |
  | :------: | :------: |
  |  weight  | 权重矩阵 |
  |   bias   | 偏置向量 |

  作用：使该全连接层的权重矩阵和偏置向量更改为输入的值

- ##### save_param方法：

  作用：返回该全连接层的权重矩阵和偏置向量

  输出：weight,bias

#### Relu类

- ##### \__init__方法

  作用：初始化参数

- ##### forward方法

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：Relu 前向传播的计算

  Relu函数的定义为：$y = max(0,x)$

  ```python
  output = np.maximum(0, input)
  ```

  输出：函数计算结果

- ##### backward方法

  | 输入参数 |          含义          |
  | :------: | :--------------------: |
  | preGrad  | 反向传播前一层的偏导数 |

  作用：Relu 反向传播的计算

  对Relu函数求导：

  $$ \frac{\partial{y}}{\partial{x}}=\left\{ \begin{matrix} 1 (x>0) \\ 0(x\leq 0)  \end{matrix} \right. $$

  经过该层后$\nabla_YL = \frac{\partial{y}}{\partial{x}}\nabla_YL$

  ```python
  postGrad[self.input < 0] = 0
  ```

​	   输出：反向传播该层的偏导数

#### Sigmoid类

- ##### \__init__方法

  作用：初始化参数

- ##### forward方法

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：Sigmoid 前向传播的计算

  Sigmoid函数的定义为：$\displaystyle y = \frac{1}{1+e^{-x}}$

  ```python
  output = 1 / (1 + np.exp(-input))
  ```

  输出：函数计算结果

- ##### backward方法

  | 输入参数 |          含义          |
  | :------: | :--------------------: |
  | preGrad  | 反向传播前一层的偏导数 |

  作用：Sigmoid 反向传播的计算

  对Sigmoid函数求导：

  $$ \displaystyle\frac{\partial{y}}{\partial{x}}= y^2e^{-x} = y(1-y)$$

  经过该层后$\displaystyle\nabla_YL = \frac{\partial{y}}{\partial{x}}\nabla_YL=y(1-y)\nabla_YL$

  ```python
  postGrad = preGrad * (1.0 - self.output) * self.output
  ```

​	   输出：反向传播该层的偏导数

#### SoftmaxWithLoss类

- ##### \__init__方法：

  作用：初始化参数

- ##### forward方法

  | 输入参数 |            含义            |
  | :------: | :------------------------: |
  |  input   | 前向传播神经网络的计算结果 |

  作用：拉大了输入值之间的差异并将其归一化为一个概率分布，便于进行多分类

  数学模型：

  $\displaystyle y_k = \frac{e^{a_k+C}}{ \displaystyle\sum_1^n e^{a_i+C}}$

  ```python
  input_max = np.max(input, axis=1, keepdims=True)
  input_exp = np.exp(input - input_max)
  self.prob = input_exp / np.sum(input_exp, axis=1, keepdims=True)
  ```

  输出：概率分布数组

- ##### get_loss方法

  | 输入参数 |   含义   |
  | :------: | :------: |
  |  label   | 标签数组 |

  作用：交叉熵误差损失函数，衡量预测值与真实值之间的差异程度，得到神经网络的性能

  交叉熵误差数学模型：

  $E = -\displaystyle \sum_{k}{t_klny_k} \\ L=-\frac{1}{p}\displaystyle \sum_{k}{t_klny_k}$

  其中$p$为样本数；$y_k$为神经网络的输出；$t_K$为正确解标签，只有正确预测的标签索引为1，其他均为0(onehot表示)

  label转化为onehot向量：

  ```python
  self.batch_size = self.prob.shape[0]
  self.label_onehot = np.zeros_like(self.prob)
  self.label_onehot[np.arange(self.batch_size), label.astype(int)] = 1.0
  ```

  计算交叉熵损失：

  ```python
  loss = -np.sum(np.log(self.prob) * self.label_onehot) / self.batch_size
  ```

  输出：交叉熵损失值

- ##### backward方法

  对softmax数学模型求偏导：

  $\displaystyle\nabla_XL = \frac{1}{p}(Y-T)$

  ```python
  postGrad = (self.prob - self.label_onehot) / self.batch_size
  ```

​		输出：反向传播该层的偏导数

#### MLP类

- ##### \__init__方法：

  |  输入参数  |                         含义                         |
  | :--------: | :--------------------------------------------------: |
  | train_data |                      训练集数组                      |
  | test_data  |                      测试集数组                      |
  | batch_size |             mini-batch 每次训练的样本数              |
  | input_size |              输入维度(MNIST数据集为64)               |
  |  hidden1   |                  第一层隐藏神经元数                  |
  |  hidden2   |                  第二层隐藏神经元数                  |
  |  out_size  |         输出维度(MNIST数据集为10：0,1,...,9)         |
  |     lr     |                    全连接层学习率                    |
  |  momentum  |                  全连接层动量超参数                  |
  | max_epoch  | 神经网络训练总次数<br />(所有训练数据均被使用过一次) |

  作用：初始化参数，建立三层神经网络结构（生成全连接层、激活函数、Softmax类的实例对象）

- ##### shuffle_data方法

  作用：打乱训练的数据集

  ```python
  np.random.shuffle(self.train_data)
  ```

- ##### init_model方法

  作用：初始化多层感知机的全连接层参数(weight,bias)

  即对网络中所有全连接层分别使用init_param()方法

- ##### forward方法

  | 输入参数 |           含义           |
  | :------: | :----------------------: |
  |  input   | mini-batch数据集图像数组 |

  作用：进行神经网络的前向传播

  每一层执行自己类的forward函数，得到的输出作为下一层的输入

  输出：神经网络预测概率数组

- ##### backward方法

  作用：进行神经网络的反向传播，得到每一层的偏导值

  反向运行：每一层执行自己类的backward函数，得到的输出作为下一层的输入，使用链式求导法则

- ##### update_param方法

  | 输入参数 |    含义    |
  | :------: | :--------: |
  |    lr    |   学习率   |
  | momentum | 动量超参数 |

  作用：更新网络中每一层全连接层的参数(weight,bias)

  即对网络中所有全连接层分别使用update_param()方法

- ##### save_param方法

  作用：保存每一层的参数(weight,bias)

  即对网络中所有全连接层分别使用save_param()方法获取参数(weight,bias)

  然后保存为文件

  ```python
  np.save('mnist.npy', params)
  ```

- ##### load_param方法

  作用：读取保存的参数(weight,bias)，并分别加载到每一层

  即对网络中所有全连接层分别使用load_param()方法加载参数(weight,bias)

- ##### train方法

  作用：使用mini-batch学习训练神经网络模型

  mini-batch学习：

  - 从训练集中选出batch_size个图片、标签
  - 用选出的训练集图片进行一次前向传播
  - 用选出的训练集标签与网络输出的预测值进行一次损失函数计算
  - 进行一次反向传播
  - 更新权重

  重复以上过程，直到所有训练数据均被使用过一次，即为一个epoch

  每个epoch运行时先使用shuffle_data()方法打乱数据

  共运行max_epoch次

  输出：每轮训练的损失值

- ##### evaluate方法

  作用：使用测试集数据运行训练后的神经网络模型，评价模型的准确度

  - 用测试集图片进行一次前向传播，得到预测概率
  - 用概率最大的项表示神经网络的预测分类标签
    pred_labels = np.argmax(prob, axis=1)
  - 计算预测分类标签的准确率
    accuracy = np.mean(pred_labels == self.test_data[:, -1])

​		输出：神经网络分类准确率

#### MNIST类

- ##### \__call__方法：

  作用：调用load_data()方法

- ##### load_data方法：

  @staticmethod 声明为静态函数，可直接调用该方法(不生成对象实例)

  作用：读取和预处理 MNIST 中训练数据和测试数据的图像和标签

  使用sklearn得到并预处理MNIST数据：

  - 使用datasets.load_digits()获得全部数据
  - 使用model_selection.train_test_split()将数据以8:2的比例分为训练集与测试集
  - 使用np.append()将图像对应标签置于最后一列，方便使用

  输出：训练集数据，测试集数据

#### main函数

作用：调用神经网络模型完成 mnist 数据集上多分类任务

- 使用MNIST类中load_data方法获得MNIST训练集与测试集
- 创建MLP类实例对象
- 调用MLP类的init_model()方法初始化神经网络
- 调用MLP类的train()方法训练神经网络
- 调用MLP类的evaluate()方法得到该神经网络分类准确度

### 3. 代码实现逻辑

**代码整体类，方法，函数调用图：**

![](https://raw.githubusercontent.com/yi226/picture/main/neutral.png)

**代码首先运行main()**

**main()函数中:**

1. 使用MNIST类中的load_data()方法获得训练集与测试集数据

2. 创建MLP类实例对象，调用MLP类\__init__方法，建立如下图所示的三层神经网络：

   ![](https://raw.githubusercontent.com/yi226/picture/main/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

   其中：

   layer1，layer2，layer3 为Linear类的实例对象

   relu1，relu2 为Relu类的实例对象

   softmax 为SoftmaxWithLoss类的实例对象

   隐藏层分别有256,128个神经元

3. 调用MLP对象init_model方法，初始化全连接层参数(weight,bias)

4. 调用MLP对象train方法，进行模型训练:
   1. 调用MLP类shuffle_data方法，打乱数据
   2. 调用MLP类forward方法，进行分类：
      按顺序调用layer1,relu1,layer2,relu2,layer3,softmax对应类的forward方法
      前一层的输出结果为该层的输入
   3. 调用softmax的get_loss方法得到损失量和onehot型标签向量
   4. 调用MLP类backward方法：
      按顺序调用softmax,layer3,relu2,layer2,relu1,layer1对应类的backward方法
      前一层的输出结果为该层的输入
   5. 调用MLP类update_param方法：
      调用layer1,layer2,layer3(Linear类)的update_param方法
   6. 重复1，2，3，4，5
   
5. 调用MLP对象evaluate方法，对训练的模型进行评价:
   1. 调用MLP类forward方法，进行分类，得到分类结果：
      按顺序调用layer1,relu1,layer2,relu2,layer3,softmax对应类的forward方法
      前一层的输出结果为该层的输入
   2. 将结果与正确值进行比较得到准确率

6. 保存神经网络参数
  
   - 调用MLP类save_param方法：
   
     1. 调用layer1,layer2,layer3的save_param方法得到参数
     2. 将参数保存为文件

7. 加载神经网络参数
   - 调用MLP类load_param方法：
      1. 读取参数文件
      2. 调用layer1,layer2,layer3的load_param方法加载参数

### 4. 模型训练过程与结果

**loss值变化：**

![](https://raw.githubusercontent.com/yi226/picture/main/Figure_1.png)

loss值随着训练的进行，先很快下降，后开始小幅度波动，并趋于0

**运行结果：**

> @Project ：Neural_Networks
> @File    ：Neural_network.py
> @Author  ：易鹏飞
> @Content ：numpy神经网络完成mnist数据集上多分类任务
>
> Epoch 0, loss: 2.209531
> Epoch 1, loss: 0.639668
> Epoch 2, loss: 0.250944
> Epoch 3, loss: 0.104238
> Epoch 4, loss: 0.153058
> ......
> Epoch 27, loss: 0.001262
> Epoch 28, loss: 0.000188
> Epoch 29, loss: 0.000348
> Accuracy in the test set: 0.986111

正确率：$98.6111\%$

## 三. 卷积神经网络(CNN.py)

### 1. 题目

使用numpy实现卷积神经网络，完成手写数字mnist数据集上多分类任务

### 2. 类，方法，函数说明

> N 数量； C 通道数； W 图像的宽； H 图像的高

#### 整个代码包括4个类(+全连接层神经网络中5个类)和2个函数：

|      类/函数       |                  主要作用                   |
| :----------------: | :-----------------------------------------: |
|   Conv(objetct)    |                  卷积层类                   |
|  Pooling(object)   |                 MAX池化层类                 |
|   Im_col(object)   |             im2col与col2im变换              |
|      CNN(MLP)      |         构建卷积神经网络(继承MLP类)         |
| reshape_data(data) |       将mnist图片转化为(N,C,W,H)形式        |
|       main()       | 调用神经网络模型完成 mnist 数据集上分类任务 |

#### Conv类

- ##### \__init__方法

  |   输入参数   |         含义          |
  | :----------: | :-------------------: |
  | kernel_shape | 卷积核的形状(N,C,W,H) |
  |    stride    |         步长          |
  |     pad      |         填充          |

  作用：初始化参数

- ##### init_param方法

  | 输入参数 |    含义    |
  | :------: | :--------: |
  |   std    | 标准化系数 |

  作用：卷积核和偏置向量初始化值

  kernel初始化为正态分布的四维数组(N,C,W,H)

  bias初始化为全为0的一维向量(N)

  ```python
  self.kernel = std * np.random.randn(*self.kernel_shape)
  self.bias = np.zeros(self.kernel_shape[0])
  ```

- ##### forward方法

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：卷积层前向传播

  数学模型：$Y = X*K+b$

  其中$*$表示卷积，K为卷积核，b为偏置向量

  通过im2col展开将卷积变化为矩阵的乘法，再更改输出的形状，简化计算

  ```python
  out = np.dot(self.col, self.col_kernel) + self.bias
  out = out.reshape(N, out_w, out_h, -1).transpose(0, 3, 1, 2)
  ```

  输出：卷积计算结果

- ##### backward方法

  | 输入参数 |          含义          |
  | :------: | :--------------------: |
  |  d_out   | 反向传播前一层的偏导数 |

  作用：卷积层反向传播，计算偏导数(梯度)
  
  转化为col形式后各参数的偏导数形式与全连接层相同
  
  相同形式计算后再转化为原来的形状
  
  ```python
  self.d_bias = np.sum(d_out, axis=0)
  self.d_kernel = np.dot(self.col.T, d_out)
  d_col = np.dot(d_out, self.col_kernel.T)
  ```
  
  输出：反向传播该层偏导数
  
- ##### update_param方法

  | 输入参数 |  含义  |
  | :------: | :----: |
  |    lr    | 学习率 |

  作用：更新参数kernel,bias(SGD方法)

  梯度下降法更新参数

  ```python
  self.kernel -= lr * self.d_kernel
  self.bias -= lr * self.d_bias
  ```

- ##### load_param方法

  | 输入参数 |   含义   |
  | :------: | :------: |
  |  kernel  |  卷积核  |
  |   bias   | 偏置向量 |

  作用：使该卷积层的卷积核和偏置向量更改为输入的值

- ##### save_param方法

  作用：返回该卷积层的卷积核和偏置向量

  输出：kernel,bias

#### Pooling类

- ##### \__init__方法

  | 输入参数 |    含义    |
  | :------: | :--------: |
  |  pool_w  | 目标区域宽 |
  |  pool_h  | 目标区域高 |
  |  stride  |    步长    |
  |   pad    |    填充    |

  作用：初始化参数

- ##### forward方法

  | 输入参数 |     含义     |
  | :------: | :----------: |
  |  input   | 输入数据数组 |

  作用：池化层前向传播

  使用im2col展开输入的数据数组，在目标区域内选出最大值，缩小W,H方向上的空间

  输出：处理后数据数组

- ##### backward方法

  | 输入参数 |          含义          |
  | :------: | :--------------------: |
  |  d_out   | 反向传播前一层的偏导数 |

  作用：池化层反向传播
  
  池化层反向传播与Relu层类似
  
  正向传播时的最大值对应位置，反向传播时等于前一层偏导数的值
  
  正向传播时不为最大值的位置，反向传播时偏导数变为0
  
  将偏导数W,H扩大为forward中输入的形状
  
  输出：反向传播该层的偏导数

#### Im_col类

- ##### im2col方法

  | 输入参数 |             含义              |
  | :------: | :---------------------------: |
  |  input   | 输入图像数组shape = (N,C,W,H) |
  | kernel_w |          卷积核的宽           |
  | kernel_h |          卷积核的高           |
  |  stride  |             步长              |
  |   pad    |             填充              |

  作用：将图像数组转化为矩阵

  首先对图像数组进行填充

  将每个图像中每次卷积的部分转换为单个向量，按正常卷积的次序排列

  输出：转换的矩阵

- ##### col2im方法

  | 输入参数 |              含义              |
  | :------: | :----------------------------: |
  |   col    |            二维数组            |
  |  input   | 转换为图像数组的形状 (N,C,W,H) |
  | kernel_w |           卷积核的宽           |
  | kernel_h |           卷积核的高           |
  |  stride  |              步长              |
  |   pad    |              填充              |

  作用：将矩阵转化为图像数组

  即im2col的反向

  输出：对应图像数组

#### CNN类

> CNN类继承MLP类(多层感知机)init_model方法，shuffle_data方法，forward方法，backward方法，update_param方法，load_param方法，save_param方法

- ##### \__init__方法

  |  输入参数  |                         含义                         |
  | :--------: | :--------------------------------------------------: |
  | train_data |                      训练集数组                      |
  | test_data  |                      测试集数组                      |
  | batch_size |             mini-batch 每次训练的样本数              |
  | input_size |            输入维度(MNIST数据集为(1,8,8))            |
  | conv_param |        卷积层参数(卷积核形状，卷积步长，填充)        |
  |  hidden1   |                 全连接层隐藏神经元数                 |
  |  out_size  |         输出维度(MNIST数据集为10：0,1,...,9)         |
  |     lr     |                    全连接层学习率                    |
  |  momentum  |                  全连接层动量超参数                  |
  | max_epoch  | 神经网络训练总次数<br />(所有训练数据均被使用过一次) |

  作用：初始化参数，建立卷积神经网络

- ##### train方法

  该方法与MLP类(多层感知机)train方法一样

  除调用forward方法前先使用reshape_data函数更改数据结构

- ##### evaluate方法

  该方法与MLP类(多层感知机)evaluate方法一样

  除调用forward方法前先使用reshape_data函数更改数据结构

#### reshape_data函数

| 输入参数 |   含义   |
| :------: | :------: |
|   data   | 图像数据 |

作用：将mnist的图片转化为四维数组shape = (N,C,W,H)

scikit-learn提供的图片：通道数=1，宽=8，高=8

输出：四维数组shape = (N,C,W,H)

#### main函数

作用：用卷积神经网络模型完成 mnist 数据集上多分类任务

与全连接层神经网络训练main函数相似

生成CNN类实例对象，对CNN对象执行相应方法

### 3. 代码实现逻辑

**代码整体类，方法，函数调用图：**

![](https://raw.githubusercontent.com/yi226/picture/main/CNN_net.png)

**代码首先运行main()**

**main()函数中:**

与使用全连接层神经网络完成 mnist 数据集上多分类任务相似

创建实例对象变为CNN类的实例对象，分别执行CNN类的对应方法

创建的卷积神经网络如下图：

![](https://raw.githubusercontent.com/yi226/picture/main/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

### 4. 模型训练过程与结果

**loss值变化：**

![](https://raw.githubusercontent.com/yi226/picture/main/cnn.png)

loss值随着训练的进行，先很快下降，后开始小幅度波动，并趋于0

**运行结果：**

> @Project ：Neural_Networks
> @File    ：CNN.py
> @Author  ：易鹏飞
> @Content ：numpy卷积神经网络完成mnist数据集上多分类任务
>
> Epoch 0, loss: 2.301806
> Epoch 1, loss: 2.269914
> Epoch 2, loss: 1.361115
> Epoch 3, loss: 0.572207
> Epoch 4, loss: 0.152905
> Epoch 5, loss: 0.300032
> ......
> Epoch 95, loss: 0.000431
> Epoch 96, loss: 0.000585
> Epoch 97, loss: 0.000114
> Epoch 98, loss: 0.000779
> Epoch 99, loss: 0.000489
> Accuracy in the test set: 0.994444

正确率：$99.4444\%$
